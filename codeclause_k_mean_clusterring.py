# -*- coding: utf-8 -*-
"""Codeclause K-mean clusterring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1036CI5QSy-zDQ-qS6tAJ7vfot7lb3ETg
"""

import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Download necessary NLTK data
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

from google.colab import files

# Upload the file
uploaded = files.upload()

# Load the data
data = pd.read_csv('IMDB Dataset.csv')
data.head()

data['review'] = data['review'].str.lower()

# Remove punctuation
data['review'] = data['review'].apply(lambda x: re.sub(r'[^\w\s]', '', x))

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

data['cleaned_review'] = data['review'].apply(preprocess_text)

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(data['cleaned_review'])

data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)

X_train, X_test, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
print(classification_report(y_test, y_pred))

data['sentiment_score'] = model.predict_proba(X)[:, 1]
data['review_length'] = data['review'].apply(lambda x: len(x.split()))
features = data[['sentiment_score', 'review_length']]
inertia = []
for n in range(1, 11):
    kmeans = KMeans(n_clusters=n, random_state=42)
    kmeans.fit(features)
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia)
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal Number of Clusters')
plt.show()

optimal_clusters = 4  # Replace with the number of clusters you find optimal
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
data['cluster'] = kmeans.fit_predict(features)

# Analyze the clusters
cluster_analysis = data.groupby('cluster').mean()
print(cluster_analysis)

data.to_csv('movie_reviews_with_clusters.csv', index=False)

from google.colab import files
files.download('movie_reviews_with_clusters.csv')